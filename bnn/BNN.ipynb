{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from blitz.modules import BayesianLinear\n",
    "from blitz.utils import variational_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@variational_estimator\n",
    "class BayesianSAPLMA(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(BayesianSAPLMA, self).__init__()\n",
    "\n",
    "        self.network = nn.Sequential(\n",
    "            BayesianLinear(input_size, 256),  \n",
    "            nn.ReLU(),\n",
    "            BayesianLinear(256, 128),        \n",
    "            nn.ReLU(),\n",
    "            BayesianLinear(128, 64),         \n",
    "            nn.ReLU(),\n",
    "            BayesianLinear(64, 1),           \n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "def train_classifier(classifier, train_loader, optimizer, criterion, epochs=5, device=\"cuda\"):\n",
    "    classifier.train()\n",
    "\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for batch in train_loader:\n",
    "            X_batch, y_batch = batch\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()           \n",
    "            loss = classifier.sample_elbo(\n",
    "                inputs=X_batch,\n",
    "                labels=y_batch,\n",
    "                criterion=lambda preds, targets: criterion(preds.squeeze(), targets),\n",
    "                sample_nbr=10, \n",
    "            )\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classifier(classifier, test_loader, criterion, device=\"cuda\", num_samples=10):\n",
    "    classifier.eval() \n",
    "    total_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():  \n",
    "        for batch in test_loader:\n",
    "            X_batch, y_batch = batch\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "           \n",
    "            outputs = [classifier(X_batch) for _ in range(num_samples)]\n",
    "            outputs = torch.stack(outputs)  \n",
    "            pred_mean = outputs.mean(dim=0).flatten()  \n",
    "\n",
    "           \n",
    "            loss = criterion(pred_mean, y_batch)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            \n",
    "            all_predictions.extend((pred_mean > 0.5).cpu().numpy()) \n",
    "            all_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Number of informative, redundant and repeated features must sum to less than the number of total features",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Generate synthetic data\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[43mmake_classification\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;66;43;03m# Number of samples\u001b[39;49;00m\n\u001b[0;32m      6\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mn_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4096\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;66;43;03m# Total number of features\u001b[39;49;00m\n\u001b[0;32m      7\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mn_informative\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Number of informative features\u001b[39;49;00m\n\u001b[0;32m      8\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mn_redundant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;66;43;03m# Number of redundant features\u001b[39;49;00m\n\u001b[0;32m      9\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mn_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;66;43;03m# Number of classes (binary)\u001b[39;49;00m\n\u001b[0;32m     10\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m11\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Convert to DataFrame for easier handling\u001b[39;00m\n\u001b[0;32m     13\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(X, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m4096\u001b[39m)])\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3.2\\envs\\ML\\lib\\site-packages\\sklearn\\datasets\\_samples_generator.py:178\u001b[0m, in \u001b[0;36mmake_classification\u001b[1;34m(n_samples, n_features, n_informative, n_redundant, n_repeated, n_classes, n_clusters_per_class, weights, flip_y, class_sep, hypercube, shift, scale, shuffle, random_state)\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;66;03m# Count features, clusters and samples\u001b[39;00m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_informative \u001b[38;5;241m+\u001b[39m n_redundant \u001b[38;5;241m+\u001b[39m n_repeated \u001b[38;5;241m>\u001b[39m n_features:\n\u001b[1;32m--> 178\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    179\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of informative, redundant and repeated \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    180\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures must sum to less than the number of total\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    181\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m features\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    182\u001b[0m     )\n\u001b[0;32m    183\u001b[0m \u001b[38;5;66;03m# Use log2 to avoid overflow errors\u001b[39;00m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_informative \u001b[38;5;241m<\u001b[39m np\u001b[38;5;241m.\u001b[39mlog2(n_classes \u001b[38;5;241m*\u001b[39m n_clusters_per_class):\n",
      "\u001b[1;31mValueError\u001b[0m: Number of informative, redundant and repeated features must sum to less than the number of total features"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "import pandas as pd\n",
    "\n",
    "# Generate synthetic data\n",
    "X, y = make_classification(n_samples=20000,     # Number of samples\n",
    "                           n_features=4096,     # Total number of features\n",
    "                           n_informative=2000,  # Number of informative features\n",
    "                           n_redundant=1000,     # Number of redundant features\n",
    "                           n_classes=2,       # Number of classes (binary)\n",
    "                           random_state=11)\n",
    "\n",
    "# Convert to DataFrame for easier handling\n",
    "df = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(4096)])\n",
    "df['label'] = y\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32)  \n",
    "\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)  \n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "input_size = 4096  # Number of features\n",
    "\n",
    "# Initialize model\n",
    "model = BayesianSAPLMA(input_size=input_size)\n",
    "model.to(\"cuda\")  # Move to GPU if available\n",
    "\n",
    "# Optimizer and criterion\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.BCELoss()  # Binary Cross-Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 4436299.5\n",
      "Epoch 2/10, Loss: 4384578.0\n",
      "Epoch 3/10, Loss: 4332624.5\n",
      "Epoch 4/10, Loss: 4281688.0\n",
      "Epoch 5/10, Loss: 4229846.5\n",
      "Epoch 6/10, Loss: 4178332.0\n",
      "Epoch 7/10, Loss: 4126459.25\n",
      "Epoch 8/10, Loss: 4074851.75\n",
      "Epoch 9/10, Loss: 4023519.25\n",
      "Epoch 10/10, Loss: 3972338.0\n"
     ]
    }
   ],
   "source": [
    "train_classifier(\n",
    "    classifier=model,\n",
    "    train_loader=train_loader,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    epochs=10,  # Number of epochs\n",
    "    device=\"cuda\"  # Use \"cuda\" if GPU is available, otherwise \"cpu\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6931\n",
      "Test Accuracy: 50.05%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = evaluate_classifier(\n",
    "    classifier=model,\n",
    "    test_loader=test_loader,\n",
    "    criterion=criterion,\n",
    "    device=\"cuda\",\n",
    "    num_samples=10\n",
    ")\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
